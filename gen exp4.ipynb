{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "084b217f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b022eb7",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4bc0ee0",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=Mgyr7EY7X9\",\n",
    "    \"https://openreview.net/pdf?id=RpQya1h1dh\",\n",
    "    \"https://openreview.net/pdf?id=ptiLVPeOWD\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"formalspecific.pdf\",\n",
    "    \"Responsibility.pdf\",\n",
    "    \"RightReasons.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071c1131",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: formalspecific.pdf\n",
      "Getting tools for paper: Responsibility.pdf\n",
      "Getting tools for paper: RightReasons.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50a802a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c1b57f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc60201",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b46ecf",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c7c3e0",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: what is The need for formal specifications of morally relevant information?\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_formalspecific with args: {\"query\": \"The need for formal specifications of morally relevant information\"}\n",
      "=== Function Output ===\n",
      "The need for formal specifications of morally relevant information arises from the limitations of informal specifications in enabling automated moral reasoning and algorithmic decision-making. Formal specifications are essential to go beyond simply comparing choices based on values and to provide a theory explaining why one option is morally superior. They help in resolving conflicts by introducing more structure, such as argumentation theory, and are crucial for justifying the morality of an option in morally conflicting situations. Additionally, formal specifications clarify the semantics and distinctions between values, factors, principles, norms, moral judgments, and moral decisions, aiding in disciplinary clarity and facilitating comparison with related work in fields like machine ethics and AI alignment.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_formalspecific with args: {\"input\": \"The need for formal specifications of morally relevant information arises from the limitations of informal specifications in enabling automated moral reasoning and algorithmic decision-making. Formal specifications are essential to go beyond simply comparing choices based on values and to provide a theory explaining why one option is morally superior. They help in resolving conflicts by introducing more structure, such as argumentation theory, and are crucial for justifying the morality of an option in morally conflicting situations. Additionally, formal specifications clarify the semantics and distinctions between values, factors, principles, norms, moral judgments, and moral decisions, aiding in disciplinary clarity and facilitating comparison with related work in fields like machine ethics and AI alignment.\"}\n",
      "=== Function Output ===\n",
      "Formal specifications of morally relevant information are crucial for enabling automated moral reasoning and algorithmic decision-making. They provide a structured framework that surpasses simple value-based comparisons, aiding in resolving conflicts, justifying moral superiority, and clarifying the distinctions between various moral aspects. This clarity enhances disciplinary understanding and allows for comparisons with related work in fields such as machine ethics and AI alignment.\n",
      "=== LLM Response ===\n",
      "The need for formal specifications of morally relevant information is essential for enabling automated moral reasoning and algorithmic decision-making. Formal specifications provide a structured framework that goes beyond simple value-based comparisons, aiding in resolving conflicts, justifying moral superiority, and clarifying the distinctions between various moral aspects. This clarity enhances disciplinary understanding and allows for comparisons with related work in fields such as machine ethics and AI alignment.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"what is The need for formal specifications of morally relevant information?\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b30872d7",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of the three pdfs\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_formalspecific with args: {\"input\": \"Summary of the first PDF\"}\n",
      "=== Function Output ===\n",
      "The first PDF delves into the significance of developing formal specifications for morally relevant information in algorithmic decision-making, stressing the importance of machines possessing moral sensitivity and making ethical choices. It advocates for the creation of formal specifications to represent morally relevant information for machines, particularly in the context of automating moral reasoning and decision-making in AI systems. The document also discusses the necessity of structured approaches like argumentation theory to address conflicts in moral decision-making, highlighting the distinctions between values, norms, moral judgments, and principles. Furthermore, it examines the impact of collective decisions on AI agents' moral behavior, the challenges in verifying and validating ethical behavior in AI systems, the role of social choice in aggregating moral preferences, and the dynamic specification of moral behavior in computational artifacts.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_Responsibility with args: {\"input\": \"Summary of the second PDF\"}\n",
      "=== Function Output ===\n",
      "The second PDF delves into the concept of responsibility within autonomous systems, focusing on forward responsibility and its impact on decision-making processes. It discusses responsibility structures and hierarchies within agent-based systems, emphasizing the role of responsibility in agent actions and inactions. The document also explores the formalization of responsibility in agents, including responsibility for achieving goals, responsibility to other agents, indifference, recklessness, and maliciousness. It touches on scenarios where agents may choose not to act despite having some responsibility and the importance of logical formulations in representing these concepts within agent systems. Additionally, the PDF discusses topics such as verifiable autonomous systems, delegation within hierarchies, certification of reliable autonomous systems, and the logic of responsibility attribution in agent organizations.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_RightReasons with args: {\"input\": \"Summary of the third PDF\"}\n",
      "=== Function Output ===\n",
      "The third PDF focuses on implementing a reason-based shield generator in a reinforcement learning architecture to guide an agent's moral decision-making. It introduces a formalism based on a fixed priority default theory to derive moral obligations from normative reasons. The shield generator restricts the agent's actions to morally permissible ones based on normative reasons and incorporates a reason theory to ensure the agent's actions align with valid normative reasons. The PDF also discusses the role of a moral judge in providing feedback on morally impermissible actions and helping update the agent's reason theory iteratively. The overall goal is to enable autonomous systems to make morally justifiable decisions in real-world contexts by continuously updating the reason theory based on the environment's state and labels.\n",
      "=== LLM Response ===\n",
      "Here are the summaries of the three PDFs:\n",
      "\n",
      "1. The first PDF discusses the importance of developing formal specifications for morally relevant information in algorithmic decision-making. It emphasizes the need for machines to possess moral sensitivity and make ethical choices. The document advocates for creating formal specifications to represent morally relevant information for machines, especially in automating moral reasoning in AI systems. It also explores structured approaches like argumentation theory to address conflicts in moral decision-making and examines the impact of collective decisions on AI agents' moral behavior.\n",
      "\n",
      "2. The second PDF explores the concept of responsibility within autonomous systems, focusing on forward responsibility and its influence on decision-making processes. It discusses responsibility structures and hierarchies within agent-based systems, highlighting the role of responsibility in agent actions and inactions. The document also delves into formalizing responsibility in agents, including responsibility for achieving goals, responsibility to other agents, indifference, recklessness, and maliciousness. It touches on scenarios where agents may choose not to act despite having some responsibility and the importance of logical formulations in representing these concepts within agent systems.\n",
      "\n",
      "3. The third PDF focuses on implementing a reason-based shield generator in a reinforcement learning architecture to guide an agent's moral decision-making. It introduces a formalism based on a fixed priority default theory to derive moral obligations from normative reasons. The shield generator restricts the agent's actions to morally permissible ones based on normative reasons and incorporates a reason theory to ensure the agent's actions align with valid normative reasons. The PDF also discusses the role of a moral judge in providing feedback on morally impermissible actions and helping update the agent's reason theory iteratively. The goal is to enable autonomous systems to make morally justifiable decisions by continuously updating the reason theory based on the environment's state and labels.\n",
      "assistant: Here are the summaries of the three PDFs:\n",
      "\n",
      "1. The first PDF discusses the importance of developing formal specifications for morally relevant information in algorithmic decision-making. It emphasizes the need for machines to possess moral sensitivity and make ethical choices. The document advocates for creating formal specifications to represent morally relevant information for machines, especially in automating moral reasoning in AI systems. It also explores structured approaches like argumentation theory to address conflicts in moral decision-making and examines the impact of collective decisions on AI agents' moral behavior.\n",
      "\n",
      "2. The second PDF explores the concept of responsibility within autonomous systems, focusing on forward responsibility and its influence on decision-making processes. It discusses responsibility structures and hierarchies within agent-based systems, highlighting the role of responsibility in agent actions and inactions. The document also delves into formalizing responsibility in agents, including responsibility for achieving goals, responsibility to other agents, indifference, recklessness, and maliciousness. It touches on scenarios where agents may choose not to act despite having some responsibility and the importance of logical formulations in representing these concepts within agent systems.\n",
      "\n",
      "3. The third PDF focuses on implementing a reason-based shield generator in a reinforcement learning architecture to guide an agent's moral decision-making. It introduces a formalism based on a fixed priority default theory to derive moral obligations from normative reasons. The shield generator restricts the agent's actions to morally permissible ones based on normative reasons and incorporates a reason theory to ensure the agent's actions align with valid normative reasons. The PDF also discusses the role of a moral judge in providing feedback on morally impermissible actions and helping update the agent's reason theory iteratively. The goal is to enable autonomous systems to make morally justifiable decisions by continuously updating the reason theory based on the environment's state and labels.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of the three pdfs\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ac140",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bbc37",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
